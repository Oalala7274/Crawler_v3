"""
Aæ¨¡å—ï¼šå‡†å¤‡ä»»åŠ¡
è¯»å–ExcelæŽ§åˆ¶å°ï¼Œç”ŸæˆJSONLæ ¼å¼çš„ä»»åŠ¡åˆ—è¡¨
"""
import os
import uuid
from typing import List
import openpyxl

from utils.models import CrawlTask
from utils.config_loader import ConfigLoader

# ä¸´æ—¶æ–‡ä»¶ç›®å½•
TEMP_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'temp')


def run(config: dict) -> List[CrawlTask]:
    """
    è¯»å–Excelå¹¶ç”Ÿæˆä»»åŠ¡åˆ—è¡¨
    
    Args:
        config: é…ç½®å­—å…¸
        
    Returns:
        CrawlTaskåˆ—è¡¨
        
    åŒæ—¶å†™å…¥ temp/tasks.jsonl
    """
    # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨
    os.makedirs(TEMP_DIR, exist_ok=True)
    
    # èŽ·å–Excelæ–‡ä»¶è·¯å¾„
    excel_file = config['settings'].get('SOURCE_EXCEL_FILE', 'å›½é™…éš”è†œä¿¡æ¯æº.xlsx')
    
    # å°è¯•åœ¨å¤šä¸ªä½ç½®æŸ¥æ‰¾Excelæ–‡ä»¶
    possible_paths = [
        excel_file,
        os.path.join(os.path.dirname(os.path.dirname(__file__)), excel_file),
        os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data', excel_file),
    ]
    
    excel_path = None
    for path in possible_paths:
        if os.path.exists(path):
            excel_path = path
            break
    
    if not excel_path:
        print(f"âŒ æ‰¾ä¸åˆ°Excelæ–‡ä»¶: {excel_file}")
        print(f"   è¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨äºŽé¡¹ç›®æ ¹ç›®å½•")
        return []
    
    print(f"ðŸ“„ è¯»å–Excel: {excel_path}")
    
    # è¯»å–Excel
    tasks = []
    try:
        wb = openpyxl.load_workbook(excel_path, read_only=True)
        ws = wb.active
        
        # èŽ·å–è¡¨å¤´
        headers = []
        for cell in ws[1]:
            headers.append(str(cell.value).strip() if cell.value else "")
        
        # æŸ¥æ‰¾åˆ—ç´¢å¼•
        col_map = {}
        required_cols = ['Base_URL', 'Is_Enabled', 'Parser_Name', 'Action_Name', 'Search_Name', 'Filter_Name', 'Engine']
        
        for i, header in enumerate(headers):
            for col in required_cols:
                if header.lower() == col.lower():
                    col_map[col] = i
                    break
        
        # æ£€æŸ¥å¿…éœ€åˆ—
        missing_cols = [col for col in required_cols if col not in col_map]
        if missing_cols:
            print(f"âš  Excelç¼ºå°‘åˆ—: {', '.join(missing_cols)}")
            # å°è¯•æŒ‰ä½ç½®çŒœæµ‹
            if len(headers) >= 7:
                col_map = {
                    'Base_URL': 0,
                    'Is_Enabled': 1,
                    'Parser_Name': 2,
                    'Action_Name': 3,
                    'Search_Name': 4,
                    'Filter_Name': 5,
                    'Engine': 6
                }
                print("   å°è¯•æŒ‰é»˜è®¤åˆ—é¡ºåºè§£æž...")
        
        # éåŽ†æ•°æ®è¡Œ
        row_count = 0
        for row in ws.iter_rows(min_row=2, values_only=True):
            if not row or not row[col_map.get('Base_URL', 0)]:
                continue
            
            # èŽ·å–å„åˆ—å€¼
            base_url = str(row[col_map.get('Base_URL', 0)] or "").strip()
            is_enabled = str(row[col_map.get('Is_Enabled', 1)] or "").upper().strip()
            parser_name = str(row[col_map.get('Parser_Name', 2)] or "").strip()
            action_name = str(row[col_map.get('Action_Name', 3)] or "WAIT").strip()
            search_name = str(row[col_map.get('Search_Name', 4)] or "NONE").strip()
            filter_name = str(row[col_map.get('Filter_Name', 5)] or "").strip()
            engine = str(row[col_map.get('Engine', 6)] or "edge").strip()
            
            # è·³è¿‡æœªå¯ç”¨çš„è¡Œ
            if is_enabled != "TRUE":
                continue
            
            row_count += 1
            
            # èŽ·å–å…³é”®è¯åˆ—è¡¨
            keywords = ConfigLoader.get_keyword_set(search_name, config)
            
            if keywords:
                # éœ€è¦æ›¿æ¢å…³é”®è¯çš„URL
                for keyword in keywords:
                    # æ›¿æ¢URLä¸­çš„å ä½ç¬¦
                    url = base_url.replace("Separator", keyword)
                    url = url.replace("{keyword}", keyword)
                    url = url.replace("%s", keyword)
                    
                    # å¯¹å…³é”®è¯è¿›è¡ŒURLç¼–ç 
                    import urllib.parse
                    encoded_keyword = urllib.parse.quote(keyword)
                    url = url.replace("{keyword_encoded}", encoded_keyword)
                    
                    task = CrawlTask(
                        task_id=uuid.uuid4().hex[:8],
                        url=url,
                        parser_name=parser_name,
                        action_name=action_name,
                        filter_name=filter_name,
                        engine=engine
                    )
                    tasks.append(task)
            else:
                # æ— éœ€æ›¿æ¢å…³é”®è¯ï¼Œç›´æŽ¥ä½¿ç”¨URL
                task = CrawlTask(
                    task_id=uuid.uuid4().hex[:8],
                    url=base_url,
                    parser_name=parser_name,
                    action_name=action_name,
                    filter_name=filter_name,
                    engine=engine
                )
                tasks.append(task)
        
        wb.close()
        print(f"   è¯»å–åˆ° {row_count} æ¡æœ‰æ•ˆé…ç½®")
        
    except Exception as e:
        print(f"âŒ è¯»å–Excelå¤±è´¥: {e}")
        return []
    
    # å†™å…¥ä»»åŠ¡æ–‡ä»¶
    tasks_file = os.path.join(TEMP_DIR, 'tasks.jsonl')
    with open(tasks_file, 'w', encoding='utf-8') as f:
        for task in tasks:
            f.write(task.to_json() + '\n')
    
    print(f"   ç”Ÿæˆ {len(tasks)} ä¸ªçˆ¬å–ä»»åŠ¡")
    print(f"   ä»»åŠ¡æ–‡ä»¶: {tasks_file}")
    
    return tasks


def load_tasks_from_file() -> List[CrawlTask]:
    """
    ä»Žä¸´æ—¶æ–‡ä»¶åŠ è½½ä»»åŠ¡åˆ—è¡¨
    
    Returns:
        CrawlTaskåˆ—è¡¨
    """
    tasks_file = os.path.join(TEMP_DIR, 'tasks.jsonl')
    tasks = []
    
    if os.path.exists(tasks_file):
        with open(tasks_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    tasks.append(CrawlTask.from_json(line))
    
    return tasks

